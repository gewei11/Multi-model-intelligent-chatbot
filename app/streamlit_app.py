import streamlit as st
import warnings
import os
import sys
import threading
import base64
from typing import Dict, Any
from PIL import Image
import io
import wave
import pyaudio
import nest_asyncio
import asyncio
import torch
import time
import torchaudio
import ChatTTS

# ç¦ç”¨æ–‡ä»¶ç›‘è§†å™¨è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)

# è®¾ç½®Streamlité…ç½®
if not os.path.exists(".streamlit"):
    os.makedirs(".streamlit")
    
with open(".streamlit/config.toml", "w") as f:
    f.write("""
[server]
fileWatcherType = "none"

[global]
developmentMode = false
""")

# è®¾ç½®é¡µé¢ - å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ªStreamlitå‘½ä»¤
st.set_page_config(
    page_title="AIåŠ©æ‰‹",
    page_icon="ğŸ¤–",
    layout="wide"
)

# æ”¹è¿›çš„äº‹ä»¶å¾ªç¯åˆå§‹åŒ–
def init_event_loop():
    try:
        loop = None
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        if not loop.is_running():
            def run_loop():
                asyncio.set_event_loop(loop)
                try:
                    loop.run_forever()
                except Exception:
                    pass
            
            thread = threading.Thread(target=run_loop, daemon=True)
            thread.start()
            
        return loop
    except Exception as e:
        st.warning(f"äº‹ä»¶å¾ªç¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return None

# åˆå§‹åŒ–äº‹ä»¶å¾ªç¯
nest_asyncio.apply()
loop = init_event_loop()

# å¤„ç† PyTorch å…¼å®¹æ€§
try:
    # å®‰å…¨åœ°è®¿é—®å’Œè®¾ç½® PyTorch é…ç½®
    if hasattr(torch, "_dynamo"):
        # ä½¿ç”¨å®‰å…¨çš„æ–¹å¼è®¿é—®å’Œè®¾ç½®å±æ€§
        dynamo_config = getattr(torch, "_dynamo", None)
        if dynamo_config and hasattr(dynamo_config, "config"):
            config_obj = getattr(dynamo_config, "config", None)
            if config_obj and hasattr(config_obj, "disable_dynamic_shapes"):
                setattr(config_obj, "disable_dynamic_shapes", True)
    
    # ç¦ç”¨æ¢¯åº¦è®¡ç®—
    if hasattr(torch, "set_grad_enabled"):
        torch.set_grad_enabled(False)
    
    # ç¦ç”¨JITç¼–è¯‘
    os.environ['PYTORCH_JIT'] = '0'
    
    # è®¾ç½®ç¡®å®šæ€§è®¡ç®—
    if hasattr(torch, "backends") and hasattr(torch.backends, "cudnn"):
        torch.backends.cudnn.deterministic = True
        # ç¦ç”¨cudnnåŸºå‡†æµ‹è¯•ä»¥æé«˜ç¨³å®šæ€§
        if hasattr(torch.backends.cudnn, "benchmark"):
            torch.backends.cudnn.benchmark = False
except Exception as e:
    st.warning(f"PyTorch é…ç½®è­¦å‘Š: {str(e)}")

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agents.core_agent import CoreAgent
from utils.config import load_config
from utils.logger import setup_logger

# è®¾ç½®ç•Œé¢æ ·å¼
st.markdown("""
<style>
    .main {
        padding: 0rem 1rem;
    }
    .stRadio [role=radiogroup] {
        gap: 1rem;
        padding: 0.5rem;
    }
    .stRadio label {
        background-color: #f0f2f6;
        padding: 0.5rem 1rem;
        border-radius: 0.5rem;
        min-width: 150px;
        text-align: center;
    }
    .stButton button {
        width: 100%;
        border-radius: 0.5rem;
        padding: 0.5rem 1rem;
        background-color: #4CAF50;
        color: white;
    }
    .stButton button:hover {
        background-color: #45a049;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
        max-width: 80%;
    }
    .user-message {
        background-color: #e3f2fd;
        margin-left: auto;
    }
    .assistant-message {
        background-color: #f5f5f5;
        margin-right: auto;
    }
    .uploaded-image {
        border-radius: 1rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# åŠ è½½é…ç½®å’Œåˆå§‹åŒ–
@st.cache_resource(show_spinner=True)
def init_agent():
    try:
        config = load_config()
        agent = CoreAgent(config)
        return agent
    except Exception as e:
        st.error(f"åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return None

# æ·»åŠ é”™è¯¯å¤„ç†
agent = init_agent()
if agent is None:
    st.error("ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å¹¶é‡è¯•ã€‚")
    st.stop()

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ£€æŸ¥å¿…è¦çš„ä¾èµ–
def check_dependencies():
    missing_deps = []
    try:
        import pyaudio
    except ImportError:
        missing_deps.append("pyaudio")
    
    if missing_deps:
        st.error(f"ç¼ºå°‘å¿…è¦çš„ä¾èµ–: {', '.join(missing_deps)}\n\n"
                 f"è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š\n"
                 f"Windows: pip install pipwin && pipwin install pyaudio\n"
                 f"å…¶ä»–ç³»ç»Ÿ: pip install pyaudio")
        return False
    return True

# ä¾§è¾¹æ è®¾ç½® - ä½¿ç”¨å®¹å™¨ç¾åŒ–å¸ƒå±€
with st.sidebar:
    st.title("âš™ï¸ è®¾ç½®")
    with st.container():
        st.subheader("æ¨¡å‹è®¾ç½®")
        model_option = st.radio(
            label="é€‰æ‹©æ¨¡å‹",  # æ·»åŠ æœ‰æ„ä¹‰çš„æ ‡ç­¾
            options=["è‡ªåŠ¨ï¼ˆæ™ºèƒ½é€‰æ‹©ï¼‰", "Qwen2.5", "DeepSeek", "æ··åˆæ¨¡å¼"],
            label_visibility="visible"  # æ˜ç¡®è®¾ç½®æ ‡ç­¾å¯è§æ€§
        )
    
    st.divider()
    
    with st.container():
        st.subheader("åŠŸèƒ½è®¾ç½®")
        input_mode = st.radio(
            label="é€‰æ‹©è¾“å…¥æ¨¡å¼",  # æ·»åŠ æœ‰æ„ä¹‰çš„æ ‡ç­¾
            options=["æ–‡æœ¬è¾“å…¥", "å›¾ç‰‡åˆ†æ", "è¯­éŸ³å¯¹è¯"],
            key="input_mode",
            label_visibility="visible"  # æ˜ç¡®è®¾ç½®æ ‡ç­¾å¯è§æ€§
        )
        st.divider()  # æ·»åŠ åˆ†éš”çº¿
        sentiment_enabled = st.toggle("æƒ…æ„Ÿåˆ†æ", value=True)
        show_analysis = st.toggle("æ˜¾ç¤ºåˆ†æç»“æœ", value=True) if sentiment_enabled else False
        weather_enabled = st.toggle("å¤©æ°”æŸ¥è¯¢", value=True)
    
    if st.button("æ¸…é™¤å¯¹è¯å†å²", type="primary"):
        st.session_state.messages = []
        agent.clear_history()
        st.experimental_rerun()

# ä¸»ç•Œé¢
st.title("ğŸ¤– AIåŠ©æ‰‹")

# æ˜¾ç¤ºèŠå¤©å†å²
st.container()  # åˆ›å»ºå®¹å™¨ç”¨äºæ˜¾ç¤ºæ¶ˆæ¯å†å²
for i, message in enumerate(st.session_state.messages):
    # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤æ¶ˆæ¯
    if i > 0 and message["content"] == st.session_state.messages[i-1]["content"]:
        continue  # è·³è¿‡é‡å¤æ¶ˆæ¯
        
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# æ ¹æ®é€‰æ‹©çš„è¾“å…¥æ¨¡å¼æ˜¾ç¤ºå¯¹åº”ç•Œé¢
if input_mode == "è¯­éŸ³å¯¹è¯":
    try:
        if check_dependencies():
            # æ·»åŠ æ–‡ä»¶ä¸Šä¼ ç»„ä»¶
            uploaded_audio = st.file_uploader(
                label="ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶",
                type=["wav"],
                help="æ”¯æŒWAVæ ¼å¼çš„éŸ³é¢‘æ–‡ä»¶"
            )
            
            # æ·»åŠ å½•éŸ³æ—¶é•¿é€‰æ‹©
            record_duration = st.radio(
                "é€‰æ‹©å½•éŸ³æ—¶é•¿",
                [5, 10, 15],
                format_func=lambda x: f"{x}ç§’",
                horizontal=True
            )
            
            # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
            if "recording" not in st.session_state:
                st.session_state.recording = False
                st.session_state.audio_frames = []
                st.session_state.pyaudio_instance = None
                st.session_state.stream = None
                st.session_state.recording_start_time = None
            
            if st.button(f"ğŸ¤ å¼€å§‹å½•éŸ³ (å°†å½•åˆ¶{record_duration}ç§’)", 
                        key="start_record",
                        disabled=st.session_state.recording):
                try:
                    # å¼€å§‹å½•éŸ³
                    CHUNK = 1024
                    FORMAT = pyaudio.paInt16
                    CHANNELS = 1
                    RATE = 16000
                    
                    p = pyaudio.PyAudio()
                    stream = p.open(format=FORMAT,
                                  channels=CHANNELS,
                                  rate=RATE,
                                  input=True,
                                  frames_per_buffer=CHUNK)
                    
                    st.session_state.recording = True
                    st.session_state.audio_frames = []
                    st.session_state.pyaudio_instance = p
                    st.session_state.stream = stream
                    st.session_state.recording_start_time = time.time()
                    
                    # æ˜¾ç¤ºå½•éŸ³è¿›åº¦
                    progress_text = st.empty()
                    progress_bar = st.progress(0)
                    
                    # å½•éŸ³å¾ªç¯ä½¿ç”¨é€‰æ‹©çš„æ—¶é•¿
                    for i in range(0, int(RATE / CHUNK * record_duration)):
                        if st.session_state.stream:
                            data = st.session_state.stream.read(CHUNK, exception_on_overflow=False)
                            st.session_state.audio_frames.append(data)
                            
                            # æ›´æ–°è¿›åº¦
                            elapsed_time = time.time() - st.session_state.recording_start_time
                            progress = min(1.0, elapsed_time / record_duration)
                            remaining_time = max(0, record_duration - elapsed_time)
                            progress_text.text(f"æ­£åœ¨å½•éŸ³... è¿˜å‰© {remaining_time:.1f} ç§’")
                            progress_bar.progress(progress)
                    
                    # å½•éŸ³å®Œæˆåè‡ªåŠ¨å¤„ç†
                    if st.session_state.recording:
                        progress_text.text("å½•éŸ³å®Œæˆï¼Œæ­£åœ¨å¤„ç†...")
                        
                        # ä¿å­˜å½•éŸ³
                        temp_file = os.path.join("temp", "temp_recording.wav")
                        os.makedirs("temp", exist_ok=True)
                        
                        with wave.open(temp_file, 'wb') as wf:
                            wf.setnchannels(CHANNELS)
                            wf.setsampwidth(p.get_sample_size(FORMAT))
                            wf.setframerate(RATE)
                            wf.writeframes(b''.join(st.session_state.audio_frames))
                        
                        # å…³é—­éŸ³é¢‘æµ
                        if st.session_state.stream:
                            st.session_state.stream.stop_stream()
                            st.session_state.stream.close()
                        if st.session_state.pyaudio_instance:
                            st.session_state.pyaudio_instance.terminate()
                        
                        # è¯†åˆ«éŸ³é¢‘
                        with st.spinner("æ­£åœ¨è¯†åˆ«è¯­éŸ³..."):
                            recognized_text = agent.voice_agent._recognize_from_file(temp_file)
                            
                            if recognized_text:
                                st.success(f"è¯†åˆ«ç»“æœ: {recognized_text}")
                                # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
                                st.session_state.messages.append({"role": "user", "content": recognized_text})
                                with st.chat_message("user"):
                                    st.markdown(recognized_text)
                                
                                # ç”ŸæˆåŠ©æ‰‹å›å¤
                                with st.chat_message("assistant"):
                                    message_placeholder = st.empty()
                                    full_response = ""
                                    
                                    # å¤„ç†è¯†åˆ«ç»“æœ
                                    for chunk in agent.process_input(
                                        recognized_text,
                                        {
                                            "model_option": model_option,
                                            "sentiment_enabled": sentiment_enabled,
                                            "show_analysis": show_analysis,
                                            "stream": True
                                        }
                                    ):
                                        full_response += chunk
                                        message_placeholder.markdown(full_response + "â–Œ")
                                    
                                    message_placeholder.markdown(full_response)
                                    # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
                                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                                    
                                    # å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³è¾“å‡º
                                    with st.spinner("æ­£åœ¨ç”Ÿæˆè¯­éŸ³..."):
                                        try:
                                            # åˆå§‹åŒ–ChatTTS
                                            chat = ChatTTS.Chat()
                                            device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
                                            chat.load(source='huggingface', device=device)
                                            
                                            # é‡‡æ ·è¯´è¯äººå’Œè®¾ç½®å‚æ•°
                                            rand_spk = chat.sample_random_speaker()
                                            params_infer_code = ChatTTS.Chat.InferCodeParams(
                                                spk_emb=rand_spk,
                                                temperature=0.3,
                                                top_P=0.7,
                                                top_K=20,
                                            )
                                            
                                            params_refine_text = ChatTTS.Chat.RefineTextParams(
                                                prompt='[oral_2][laugh_0][break_6]',
                                            )
                                            
                                            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                                            output_dir = "temp/tts_output"
                                            os.makedirs(output_dir, exist_ok=True)
                                            
                                            # ç”Ÿæˆè¯­éŸ³
                                            wavs = chat.infer(
                                                [full_response],
                                                params_refine_text=params_refine_text,
                                                params_infer_code=params_infer_code,
                                            )
                                            
                                            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
                                            output_file = f"{output_dir}/response.wav"
                                            try:
                                                torchaudio.save(output_file, torch.from_numpy(wavs[0]).unsqueeze(0), 24000)
                                            except:
                                                torchaudio.save(output_file, torch.from_numpy(wavs[0]), 24000)
                                            
                                            # è¯»å–ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶
                                            if os.path.exists(output_file):
                                                audio_file = open(output_file, 'rb')
                                                audio_bytes = audio_file.read()
                                                audio_file.close()
                                                
                                                # åœ¨ç•Œé¢ä¸Šæ˜¾ç¤ºéŸ³é¢‘æ’­æ”¾å™¨
                                                st.audio(audio_bytes, format='audio/wav')
                                                
                                                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                                                try:
                                                    os.remove(output_file)
                                                except:
                                                    pass
                                            else:
                                                st.warning("è¯­éŸ³æ–‡ä»¶ç”Ÿæˆå¤±è´¥")
                                                
                                        except Exception as e:
                                            st.error(f"è¯­éŸ³åˆæˆå¤±è´¥: {str(e)}")

                            else:
                                st.warning("æœªèƒ½è¯†åˆ«å‡ºè¯­éŸ³å†…å®¹ï¼Œè¯·é‡æ–°å½•éŸ³")
                        
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        try:
                            os.remove(temp_file)
                        except:
                            pass
                        
                        # é‡ç½®å½•éŸ³çŠ¶æ€
                        st.session_state.recording = False
                        st.session_state.audio_frames = []
                        st.session_state.stream = None
                        st.session_state.pyaudio_instance = None
                        st.session_state.recording_start_time = None
                        
                except Exception as e:
                    st.error(f"å½•éŸ³è¿‡ç¨‹å‡ºé”™: {str(e)}")
                    # ç¡®ä¿èµ„æºè¢«é‡Šæ”¾
                    if hasattr(st.session_state, 'stream') and st.session_state.stream:
                        st.session_state.stream.stop_stream()
                        st.session_state.stream.close()
                    if hasattr(st.session_state, 'pyaudio_instance') and st.session_state.pyaudio_instance:
                        st.session_state.pyaudio_instance.terminate()
                    
                    # é‡ç½®çŠ¶æ€
                    st.session_state.recording = False
                    st.session_state.audio_frames = []
                    st.session_state.stream = None
                    st.session_state.pyaudio_instance = None
                    st.session_state.recording_start_time = None

            # å¤„ç†ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶
            if uploaded_audio is not None:
                with st.spinner("å¤„ç†è¯­éŸ³ä¸­..."):
                    try:
                        result = agent.voice_agent.process_input(uploaded_audio.getvalue())
                        if "error" not in result:
                            st.success(f"è¯†åˆ«ç»“æœ: {result['recognized_text']}")
                        else:
                            st.error(result["error"])
                    except Exception as e:
                        st.error(f"å¤„ç†è¯­éŸ³å¤±è´¥: {str(e)}")

        else:
            st.info("è¯·å®‰è£…æ‰€éœ€ä¾èµ–åç»§ç»­ä½¿ç”¨è¯­éŸ³åŠŸèƒ½")

    except Exception as e:
        st.error(f"è¯­éŸ³åŠŸèƒ½å‡ºé”™: {str(e)}")

elif input_mode == "æ–‡æœ¬è¾“å…¥":
    # æ–‡æœ¬è¾“å…¥æ¨¡å¼
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...", key="text_input"):
        # é¿å…é‡å¤æ·»åŠ ç›¸åŒçš„æ¶ˆæ¯
        if not st.session_state.messages or st.session_state.messages[-1]["content"] != prompt:
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            
            # ç”ŸæˆåŠ©æ‰‹å›å¤
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""
                
                try:
                    # æ·»åŠ åŠ è½½åŠ¨ç”»
                    with st.spinner('æ€è€ƒä¸­...'):
                        # åˆ›å»ºä¸€ä¸ªç©ºçš„æ¶ˆæ¯å ä½ç¬¦
                        message_placeholder = st.empty()
                        full_response = ""
                        buffer = ""
                        last_update_time = time.time()
                        update_interval = 0.1  # æ§åˆ¶æ›´æ–°é¢‘ç‡ï¼Œæ¯0.1ç§’æ›´æ–°ä¸€æ¬¡
                        
                        for response_chunk in agent.process_input(prompt, {
                            "model_option": model_option,
                            "weather_enabled": weather_enabled,
                            "sentiment_enabled": sentiment_enabled,
                            "show_analysis": show_analysis,
                            "stream": True
                        }):
                            buffer += response_chunk
                            current_time = time.time()
                            
                            # å½“ç§¯ç´¯çš„æ–‡æœ¬è¶…è¿‡ä¸€å®šé•¿åº¦æˆ–è¾¾åˆ°æ›´æ–°é—´éš”æ—¶æ›´æ–°æ˜¾ç¤º
                            if len(buffer) >= 5 or (current_time - last_update_time) >= update_interval:
                                full_response += buffer
                                message_placeholder.markdown(full_response + "â–Œ")
                                buffer = ""
                                last_update_time = current_time
                                time.sleep(0.02)  # æ·»åŠ å°å»¶è¿Ÿä»¥å®ç°æ‰“å­—æœºæ•ˆæœ
                    
                    # ç¡®ä¿æ˜¾ç¤ºæœ€åçš„æ–‡æœ¬
                    if buffer:
                        full_response += buffer
                        message_placeholder.markdown(full_response)
                    
                    # é¿å…é‡å¤æ·»åŠ ç›¸åŒçš„å›å¤
                    if not st.session_state.messages or st.session_state.messages[-1]["content"] != full_response:
                        st.session_state.messages.append({"role": "assistant", "content": full_response})
                    
                except Exception as e:
                    error_msg = f"âš ï¸ ç”Ÿæˆå›å¤æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
                    st.error(error_msg)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})

elif input_mode == "å›¾ç‰‡åˆ†æ":
    # ä½¿ç”¨columnså¸ƒå±€ç¾åŒ–ä¸Šä¼ åŒºåŸŸ
    col1, col2, _ = st.columns([2, 2, 1])
    with col1:
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ å›¾ç‰‡",
            type=["jpg", "jpeg", "png"],
            help="æ”¯æŒjpgã€jpegã€pngæ ¼å¼çš„å›¾ç‰‡"
        )
    
    if uploaded_file is not None:
        # æ˜¾ç¤ºä¸Šä¼ çš„å›¾ç‰‡,æ·»åŠ æ ·å¼
        image = Image.open(uploaded_file)
        st.image(
            image, 
            caption="ä¸Šä¼ çš„å›¾ç‰‡", 
            use_container_width=True  # ä½¿ç”¨æ–°å‚æ•°æ›¿æ¢use_column_width
        )
        
        # ç¾åŒ–æç¤ºè¾“å…¥åŒºåŸŸ
        with st.container():
            prompt = st.text_input(
                "è¯·è¾“å…¥å…³äºå›¾ç‰‡çš„é—®é¢˜",
                value="è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ",
                placeholder="ä¾‹å¦‚:æè¿°ä¸€ä¸‹è¿™å¼ å›¾ç‰‡çš„å†…å®¹..."
            )
        
        # ç¾åŒ–åˆ†ææŒ‰é’®
        if st.button("âœ¨ å¼€å§‹åˆ†æ", key="analyze_btn"):
            try:
                # æ·»åŠ è¿›åº¦æ¡å’ŒåŠ è½½åŠ¨ç”»
                with st.spinner('æ­£åœ¨åˆ†æå›¾ç‰‡...'):
                    progress_bar = st.progress(0)
                    
                    # ä¿å­˜å›¾ç‰‡å¤„ç†
                    temp_image_path = os.path.join("temp", "temp_image.png")
                    os.makedirs("temp", exist_ok=True)
                    image.save(temp_image_path)
                    progress_bar.progress(30)

                    # æ·»åŠ åˆ°å¯¹è¯å†å²
                    st.session_state.messages.append({"role": "user", "content": f"[å›¾ç‰‡åˆ†æ] {prompt}"})
                    progress_bar.progress(50)
                    
                    # æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
                    with st.chat_message("user"):
                        st.markdown(f"ğŸ–¼ï¸ [å›¾ç‰‡] {prompt}")
                    
                    # è°ƒç”¨æ¨¡å‹åˆ†æ
                    with st.chat_message("assistant"):
                        message_placeholder = st.empty()
                        full_response = ""
                        
                        config = {
                            "models": {
                                "minicpm": {
                                    "model_name": "aiden_lu/minicpm-v2.6:Q4_K_M",
                                    "api_base": "http://localhost:11434/v1/chat/completions",
                                    "temperature": 0.7,
                                    "max_tokens": 2048,
                                    "vision": True
                                }
                            }
                        }
                        
                        from models.minicpm import MiniCPMModel
                        model = MiniCPMModel(config["models"]["minicpm"])
                        progress_bar.progress(70)
                        
                        for chunk in model.generate(prompt, images=[temp_image_path]):
                            full_response += chunk
                            message_placeholder.markdown(full_response + "â–Œ")
                        
                        message_placeholder.markdown(full_response)
                        st.session_state.messages.append({"role": "assistant", "content": full_response})
                        progress_bar.progress(100)
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    os.remove(temp_image_path)
                    
            except Exception as e:
                error_msg = f"âš ï¸ å›¾ç‰‡åˆ†æå¤±è´¥: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})